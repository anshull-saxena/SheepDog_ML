{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üì∞ RoBERTa-Based News Fake Detection with Synonym Augmentation and Reframing\n",
        "\n",
        "## Project Overview\n",
        "\n",
        "This notebook implements a deep learning model for fake news detection using **RoBERTa**, enhanced with **synonym-based text augmentation** and **fine-grained reframing supervision**. The goal is to improve model robustness against adversarial paraphrasing by training on semantically equivalent but lexically altered versions of news texts.\n",
        "\n",
        "### Key Features:\n",
        "- ‚úÖ Synonym swapping using WordNet (nouns & adjectives only)\n",
        "- ‚úÖ Triple-augmentation training: original + 2 augmented versions\n",
        "- ‚úÖ Dual-task learning: classification + fine-grained framing (binary)\n",
        "- ‚úÖ Consistency regularization via KL-divergence\n",
        "- ‚úÖ Evaluation on 4 adversarial styles: Objective, Neutral, Emotionally Triggering, Sensational\n",
        "- ‚úÖ Model checkpointing and logging\n",
        "\n",
        "### Dependencies:\n",
        "- PyTorch, Transformers, NLTK, scikit-learn\n",
        "- Custom data loaders: `load_articles()` and `load_reframing()` from `utils.load_data`\n",
        "\n",
        "Note: Ensure `utils/load_data.py` exists in the parent directory with required functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import RobertaModel, RobertaTokenizer\n",
        "from transformers.optimization import get_linear_schedule_with_warmup\n",
        "from torch.optim import AdamW\n",
        "import argparse\n",
        "import numpy as np\n",
        "import sys\n",
        "import os\n",
        "import nltk\n",
        "import random\n",
        "from sklearn.metrics import precision_recall_fscore_support as score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "\n",
        "# Suppress warnings to reduce console clutter\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Add parent directory to Python path for custom utilities\n",
        "sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n",
        "\n",
        "print(\"üîç Current working directory:\", os.getcwd())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üõ†Ô∏è 1. Download and Initialize NLTK Resources\n",
        "\n",
        "We download required natural language processing datasets for synonym replacement using WordNet and POS tagging."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download required NLTK corpora for synonym swapping and POS tagging\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('punkt_tab', quiet=True)\n",
        "nltk.download('averaged_perceptron_tagger_eng', quiet=True)\n",
        "nltk.download('wordnet', quiet=True)\n",
        "nltk.download('averaged_perceptron_tagger', quiet=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚öôÔ∏è 2. Command-Line Argument Parser\n",
        "\n",
        "Configures training parameters via CLI for reproducibility and experimentation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "parser = argparse.ArgumentParser(description='RoBERTa with Synonym Augmentation for Fake News Detection')\n",
        "parser.add_argument('--dataset_name', default='politifact', type=str, help='Name of dataset (e.g., politifact, gossipcop)')\n",
        "parser.add_argument('--model_name', default='Pretrained-LM', type=str, help='Model identifier for logging')\n",
        "parser.add_argument('--iters', default=3, type=int, help='Number of training iterations (seeds)')\n",
        "parser.add_argument('--batch_size', default=4, type=int, help='Batch size for training')\n",
        "parser.add_argument('--n_epochs', default=5, type=int, help='Number of training epochs per iteration')\n",
        "\n",
        "args = parser.parse_args()\n",
        "\n",
        "# Set device to GPU if available, otherwise fallback to CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"üöÄ Using device: {device}\")\n",
        "\n",
        "# Set random seeds for reproducibility across runs\n",
        "torch.manual_seed(0)\n",
        "np.random.seed(0)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(0)\n",
        "print(\"‚úÖ Random seeds set for reproducibility.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîÅ 3. Synonym Augmentation Function\n",
        "\n",
        "Randomly replaces 1‚Äì2 nouns or adjectives in a text with synonyms from WordNet.\n",
        "Only replaces words with valid synonyms and avoids replacing the same word multiple times."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def synonym_swap(text, n=1):\n",
        "    \"\"\"\n",
        "    Randomly replace n nouns or adjectives with synonyms using WordNet.\n",
        "    \n",
        "    Args:\n",
        "        text (str): Input news text\n",
        "        n (int): Number of words to replace (default: 1)\n",
        "    \n",
        "    Returns:\n",
        "        str: Augmented text with synonyms\n",
        "    \"\"\"\n",
        "    words = nltk.word_tokenize(text)\n",
        "    pos_tags = nltk.pos_tag(words)\n",
        "    \n",
        "    # Identify replaceable POS tags: Nouns (NN, NNS, NNP, NNPS) and Adjectives (JJ, JJR, JJS)\n",
        "    replaceable_indices = [i for i, (_, tag) in enumerate(pos_tags) \n",
        "                          if tag.startswith(('NN', 'JJ'))]\n",
        "    \n",
        "    # If no replaceable words, return original text\n",
        "    if not replaceable_indices:\n",
        "        return text\n",
        "    \n",
        "    # Limit replacements to available words\n",
        "    n = min(n, len(replaceable_indices))\n",
        "    selected_indices = random.sample(replaceable_indices, n)\n",
        "    \n",
        "    for idx in selected_indices:\n",
        "        word = words[idx]\n",
        "        synonyms = []\n",
        "        \n",
        "        # Collect unique synonyms from WordNet\n",
        "        for syn in wordnet.synsets(word):\n",
        "            for lemma in syn.lemmas():\n",
        "                synonym = lemma.name().replace('_', ' ')\n",
        "                if synonym != word and synonym not in synonyms:\n",
        "                    synonyms.append(synonym)\n",
        "        \n",
        "        # Replace word with a random synonym if available\n",
        "        if synonyms:\n",
        "            words[idx] = random.choice(synonyms)\n",
        "    \n",
        "    return ' '.join(words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üì¶ 4. Training Dataset Class: NewsDatasetAug\n",
        "\n",
        "Custom PyTorch Dataset that loads:\n",
        "- Original text\n",
        "- Two synonym-augmented versions (random 1‚Äì2 swaps)\n",
        "- Labels and fine-grained framing labels (for consistency loss)\n",
        "\n",
        "Uses RoBERTa tokenizer to encode all versions with padding and truncation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class NewsDatasetAug(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset for training with synonym augmentation and fine-grained framing supervision.\n",
        "    Each sample includes original text + 2 augmented versions with corresponding labels.\n",
        "    \"\"\"\n",
        "    def __init__(self, texts, aug_texts1, aug_texts2, labels, fg_label, aug_fg1, aug_fg2, tokenizer, max_len):\n",
        "        self.texts = texts\n",
        "        # Apply synonym swapping on-the-fly for stochastic augmentation\n",
        "        self.aug_texts1 = [synonym_swap(text, n=random.randint(1, 2)) for text in aug_texts1]\n",
        "        self.aug_texts2 = [synonym_swap(text, n=random.randint(1, 2)) for text in aug_texts2]\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "        self.labels = np.array(labels)\n",
        "        self.fg_label = np.array(fg_label)\n",
        "        self.aug_fg1 = np.array(aug_fg1)\n",
        "        self.aug_fg2 = np.array(aug_fg2)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        text = self.texts[item]\n",
        "        aug_text1 = self.aug_texts1[item]\n",
        "        aug_text2 = self.aug_texts2[item]\n",
        "        label = self.labels[item]\n",
        "        fg_label = self.fg_label[item]\n",
        "        aug_fg1 = self.aug_fg1[item]\n",
        "        aug_fg2 = self.aug_fg2[item]\n",
        "        \n",
        "        # Encode original text\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_token_type_ids=False,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        # Encode first augmented version\n",
        "        aug1_encoding = self.tokenizer.encode_plus(\n",
        "            aug_text1,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_token_type_ids=False,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        # Encode second augmented version\n",
        "        aug2_encoding = self.tokenizer.encode_plus(\n",
        "            aug_text2,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_token_type_ids=False,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'input_ids_aug1': aug1_encoding['input_ids'].flatten(),\n",
        "            'input_ids_aug2': aug2_encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'attention_mask_aug1': aug1_encoding['attention_mask'].flatten(),\n",
        "            'attention_mask_aug2': aug2_encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(label, dtype=torch.long),\n",
        "            'fg_label': torch.tensor(fg_label, dtype=torch.float),\n",
        "            'fg_label_aug1': torch.tensor(aug_fg1, dtype=torch.float),\n",
        "            'fg_label_aug2': torch.tensor(aug_fg2, dtype=torch.float),\n",
        "        }\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üì¶ 5. Evaluation Dataset Class: NewsDataset\n",
        "\n",
        "Simplified dataset class for evaluating on original (non-augmented) test sets.\n",
        "Used for reporting performance on clean, adversarial, and reframed test splits."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class NewsDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset for evaluation on original texts only.\n",
        "    Used for testing on clean, adversarial, and reframed test sets.\n",
        "    \"\"\"\n",
        "    def __init__(self, texts, labels, tokenizer, max_len):\n",
        "        self.texts = texts\n",
        "        self.labels = np.array(labels)\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        text = self.texts[item]\n",
        "        label = self.labels[item]\n",
        "        \n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_token_type_ids=False,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'news_text': text,\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üß† 6. RoBERTa Classifier Model Architecture\n",
        "\n",
        "Dual-output RoBERTa classifier:\n",
        "- Primary output: 4-class fake news classification\n",
        "- Secondary output: Binary fine-grained framing prediction (e.g., objective vs. sensational)\n",
        "\n",
        "Uses dropout and linear projections for regularization and task-specific heads."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class RobertaClassifier(nn.Module):\n",
        "    \"\"\"\n",
        "    RoBERTa-based classifier with dual outputs:\n",
        "    - Multi-class classification (4 classes: true, false, etc.)\n",
        "    - Binary framing classification (e.g., objective/neutral/emotional/sensational)\n",
        "    \"\"\"\n",
        "    def __init__(self, n_classes):\n",
        "        super(RobertaClassifier, self).__init__()\n",
        "        self.roberta = RobertaModel.from_pretrained('roberta-base')\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "        self.fc_out = nn.Linear(self.roberta.config.hidden_size, n_classes)  # 4-class output\n",
        "        self.binary_transform = nn.Linear(self.roberta.config.hidden_size, 2)  # Binary framing output\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        # Get RoBERTa pooled output\n",
        "        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled_outputs = outputs[1]  # [CLS] token embedding\n",
        "        \n",
        "        # Apply dropout for regularization\n",
        "        pooled_outputs = self.dropout(pooled_outputs)\n",
        "        \n",
        "        # Dual outputs\n",
        "        class_logits = self.fc_out(pooled_outputs)     # 4-class classification\n",
        "        framing_logits = self.binary_transform(pooled_outputs)  # Binary framing\n",
        "        \n",
        "        return class_logits, framing_logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üöö 7. Data Loaders: Training and Evaluation\n",
        "\n",
        "Factory functions to create PyTorch DataLoaders for:\n",
        "- Training: with synonym-augmented triplets\n",
        "- Evaluation: on original test sets (clean + 4 adversarial variants)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_train_loader(contents, contents_aug1, contents_aug2, labels, fg_label, aug_fg1, aug_fg2, tokenizer, max_len, batch_size):\n",
        "    \"\"\"\n",
        "    Creates a DataLoader for training with original + 2 synonym-augmented samples.\n",
        "    \"\"\"\n",
        "    ds = NewsDatasetAug(\n",
        "        texts=contents,\n",
        "        aug_texts1=contents_aug1,\n",
        "        aug_texts2=contents_aug2,\n",
        "        labels=np.array(labels),\n",
        "        fg_label=fg_label,\n",
        "        aug_fg1=aug_fg1,\n",
        "        aug_fg2=aug_fg2,\n",
        "        tokenizer=tokenizer,\n",
        "        max_len=max_len\n",
        "    )\n",
        "    return DataLoader(ds, batch_size=batch_size, shuffle=True, num_workers=0)\n",
        "\n",
        "def create_eval_loader(contents, labels, tokenizer, max_len, batch_size):\n",
        "    \"\"\"\n",
        "    Creates a DataLoader for evaluation on original texts only.\n",
        "    \"\"\"\n",
        "    ds = NewsDataset(texts=contents, labels=np.array(labels), tokenizer=tokenizer, max_len=max_len)\n",
        "    return DataLoader(ds, batch_size=batch_size, num_workers=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üî¢ 8. Seed Setter for Reproducibility\n",
        "\n",
        "Ensures consistent results across training iterations by fixing all random seeds."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def set_seed(seed):\n",
        "    \"\"\"\n",
        "    Set all random seeds for deterministic behavior.\n",
        "    \"\"\"\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    print(f\"üîÅ Seed set to {seed}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üèãÔ∏è 9. Training Function: Main Training Loop\n",
        "\n",
        "Trains the RoBERTa model using:\n",
        "- Supervised classification loss\n",
        "- Consistency loss (KL divergence between original and augmented outputs)\n",
        "- Fine-grained framing loss (BCE)\n",
        "\n",
        "Evaluates on 5 test sets: original + 4 adversarial styles.\n",
        "Saves model checkpoints and logs metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_model(tokenizer, max_len, n_epochs, batch_size, datasetname, iter):\n",
        "    \"\"\"\n",
        "    Main training loop for one iteration.\n",
        "    Loads data, trains model, evaluates on multiple test sets, saves model.\n",
        "    \"\"\"\n",
        "    print(f\"\\nüìà Starting Training Iteration {iter+1}/{args.iters} for dataset: {datasetname}\")\n",
        "    \n",
        "    # Load dataset: original and adversarial splits\n",
        "    x_train, x_test, x_test_a, x_test_b, x_test_c, x_test_d, y_train, y_test = load_articles(datasetname)\n",
        "    \n",
        "    # Load fine-grained framing labels\n",
        "    x_train_res1, x_train_res2, y_train_fg, y_train_fg_m, y_train_fg_t = load_reframing(datasetname)\n",
        "    \n",
        "    # Create data loaders\n",
        "    test_loader = create_eval_loader(x_test, y_test, tokenizer, max_len, batch_size)\n",
        "    test_loader_a = create_eval_loader(x_test_a, y_test, tokenizer, max_len, batch_size)\n",
        "    test_loader_b = create_eval_loader(x_test_b, y_test, tokenizer, max_len, batch_size)\n",
        "    test_loader_c = create_eval_loader(x_test_c, y_test, tokenizer, max_len, batch_size)\n",
        "    test_loader_d = create_eval_loader(x_test_d, y_test, tokenizer, max_len, batch_size)\n",
        "\n",
        "    # Initialize model\n",
        "    model = RobertaClassifier(n_classes=4).to(device)\n",
        "    optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "    total_steps = 10000  # Approximate total training steps\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
        "\n",
        "    train_losses = []\n",
        "    train_accs = []\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        model.train()\n",
        "        \n",
        "        # Create training loader with stochastic synonym augmentation\n",
        "        train_loader = create_train_loader(\n",
        "            x_train, x_train_res1, x_train_res2, y_train, y_train_fg, y_train_fg_m, y_train_fg_t, tokenizer, max_len, batch_size\n",
        "        )\n",
        "        \n",
        "        avg_loss = []\n",
        "        avg_acc = []\n",
        "        \n",
        "        for batch_data in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{n_epochs}\"):\n",
        "            # Move data to device\n",
        "            input_ids = batch_data[\"input_ids\"].to(device)\n",
        "            attention_mask = batch_data[\"attention_mask\"].to(device)\n",
        "            input_ids_aug1 = batch_data[\"input_ids_aug1\"].to(device)\n",
        "            attention_mask_aug1 = batch_data[\"attention_mask_aug1\"].to(device)\n",
        "            input_ids_aug2 = batch_data[\"input_ids_aug2\"].to(device)\n",
        "            attention_mask_aug2 = batch_data[\"attention_mask_aug2\"].to(device)\n",
        "            targets = batch_data[\"labels\"].to(device)\n",
        "            fg_labels = batch_data[\"fg_label\"].to(device)\n",
        "            fg_labels_aug1 = batch_data[\"fg_label_aug1\"].to(device)\n",
        "            fg_labels_aug2 = batch_data[\"fg_label_aug2\"].to(device)\n",
        "            \n",
        "            # Forward pass\n",
        "            out_labels, out_labels_bi = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            out_labels_aug1, out_labels_bi_aug1 = model(input_ids=input_ids_aug1, attention_mask=attention_mask_aug1)\n",
        "            out_labels_aug2, out_labels_bi_aug2 = model(input_ids=input_ids_aug2, attention_mask=attention_mask_aug2)\n",
        "            \n",
        "            # Loss Components\n",
        "            # 1. Fine-grained framing loss (BCE on sigmoid outputs)\n",
        "            fg_criterion = nn.BCELoss()\n",
        "            finegrain_loss = (\n",
        "                fg_criterion(torch.sigmoid(out_labels_bi), fg_labels) +\n",
        "                fg_criterion(torch.sigmoid(out_labels_bi_aug1), fg_labels_aug1) +\n",
        "                fg_criterion(torch.sigmoid(out_labels_bi_aug2), fg_labels_aug2)\n",
        "            ) / 3\n",
        "            \n",
        "            # 2. Supervised classification loss (CrossEntropy)\n",
        "            sup_criterion = nn.CrossEntropyLoss()\n",
        "            sup_loss = sup_criterion(out_labels_bi, targets)\n",
        "            \n",
        "            # 3. Consistency regularization (KL divergence between original and augmented predictions)\n",
        "            out_probs = F.softmax(out_labels_bi, dim=-1)\n",
        "            aug_log_prob1 = F.log_softmax(out_labels_bi_aug1, dim=-1)\n",
        "            aug_log_prob2 = F.log_softmax(out_labels_bi_aug2, dim=-1)\n",
        "            \n",
        "            cons_criterion = nn.KLDivLoss(reduction='batchmean')\n",
        "            cons_loss = 0.5 * cons_criterion(aug_log_prob1, out_probs) + 0.5 * cons_criterion(aug_log_prob2, out_probs)\n",
        "            \n",
        "            # Total loss\n",
        "            loss = sup_loss + cons_loss + finegrain_loss\n",
        "            \n",
        "            # Backward pass\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Gradient clipping\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            \n",
        "            # Accuracy calculation\n",
        "            _, pred = out_labels_bi.max(dim=-1)\n",
        "            correct = pred.eq(targets).sum().item()\n",
        "            train_acc = correct / len(targets)\n",
        "            \n",
        "            avg_loss.append(loss.item())\n",
        "            avg_acc.append(train_acc)\n",
        "        \n",
        "        # Log epoch metrics\n",
        "        train_losses.append(np.mean(avg_loss))\n",
        "        train_accs.append(np.mean(avg_acc))\n",
        "        print(f\"Iter {iter:03d} | Epoch {epoch+1:05d} | Train Loss {np.mean(avg_loss):.4f} | Train Acc. {np.mean(avg_acc):.4f}\")\n",
        "        \n",
        "        # Evaluate on test sets at last epoch\n",
        "        if epoch == n_epochs - 1:\n",
        "            model.eval()\n",
        "            y_pred = []\n",
        "            y_pred_a = []\n",
        "            y_pred_b = []\n",
        "            y_pred_c = []\n",
        "            y_pred_d = []\n",
        "            y_test_true = []\n",
        "            \n",
        "            # Evaluate on original test set\n",
        "            for batch_data in tqdm(test_loader, desc=\"Evaluating Original\"):\n",
        "                with torch.no_grad():\n",
        "                    input_ids = batch_data[\"input_ids\"].to(device)\n",
        "                    attention_mask = batch_data[\"attention_mask\"].to(device)\n",
        "                    targets = batch_data[\"labels\"].to(device)\n",
        "                    _, val_out = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "                    _, val_pred = val_out.max(dim=1)\n",
        "                    y_pred.append(val_pred)\n",
        "                    y_test_true.append(targets)\n",
        "            \n",
        "            # Evaluate on adversarial set A: Objective\n",
        "            for batch_data in tqdm(test_loader_a, desc=\"Evaluating A (Objective)\"):\n",
        "                with torch.no_grad():\n",
        "                    input_ids = batch_data[\"input_ids\"].to(device)\n",
        "                    attention_mask = batch_data[\"attention_mask\"].to(device)\n",
        "                    _, val_out = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "                    _, val_pred = val_out.max(dim=1)\n",
        "                    y_pred_a.append(val_pred)\n",
        "            \n",
        "            # Evaluate on adversarial set B: Neutral\n",
        "            for batch_data in tqdm(test_loader_b, desc=\"Evaluating B (Neutral)\"):\n",
        "                with torch.no_grad():\n",
        "                    input_ids = batch_data[\"input_ids\"].to(device)\n",
        "                    attention_mask = batch_data[\"attention_mask\"].to(device)\n",
        "                    _, val_out = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "                    _, val_pred = val_out.max(dim=1)\n",
        "                    y_pred_b.append(val_pred)\n",
        "            \n",
        "            # Evaluate on adversarial set C: Emotionally Triggering\n",
        "            for batch_data in tqdm(test_loader_c, desc=\"Evaluating C (Emotionally Triggering)\"):\n",
        "                with torch.no_grad():\n",
        "                    input_ids = batch_data[\"input_ids\"].to(device)\n",
        "                    attention_mask = batch_data[\"attention_mask\"].to(device)\n",
        "                    _, val_out = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "                    _, val_pred = val_out.max(dim=1)\n",
        "                    y_pred_c.append(val_pred)\n",
        "            \n",
        "            # Evaluate on adversarial set D: Sensational\n",
        "            for batch_data in tqdm(test_loader_d, desc=\"Evaluating D (Sensational)\"):\n",
        "                with torch.no_grad():\n",
        "                    input_ids = batch_data[\"input_ids\"].to(device)\n",
        "                    attention_mask = batch_data[\"attention_mask\"].to(device)\n",
        "                    _, val_out = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "                    _, val_pred = val_out.max(dim=1)\n",
        "                    y_pred_d.append(val_pred)\n",
        "            \n",
        "            # Concatenate predictions\n",
        "            y_pred = torch.cat(y_pred, dim=0)\n",
        "            y_pred_a = torch.cat(y_pred_a, dim=0)\n",
        "            y_pred_b = torch.cat(y_pred_b, dim=0)\n",
        "            y_pred_c = torch.cat(y_pred_c, dim=0)\n",
        "            y_pred_d = torch.cat(y_pred_d, dim=0)\n",
        "            y_test_true = torch.cat(y_test_true, dim=0)\n",
        "            \n",
        "            # Compute metrics\n",
        "            acc = accuracy_score(y_test_true.cpu().numpy(), y_pred.cpu().numpy())\n",
        "            precision, recall, fscore, _ = score(y_test_true.cpu().numpy(), y_pred.cpu().numpy(), average='macro')\n",
        "            \n",
        "            acc_a = accuracy_score(y_test_true.cpu().numpy(), y_pred_a.cpu().numpy())\n",
        "            precision_a, recall_a, fscore_a, _ = score(y_test_true.cpu().numpy(), y_pred_a.cpu().numpy(), average='macro')\n",
        "            \n",
        "            acc_b = accuracy_score(y_test_true.cpu().numpy(), y_pred_b.cpu().numpy())\n",
        "            precision_b, recall_b, fscore_b, _ = score(y_test_true.cpu().numpy(), y_pred_b.cpu().numpy(), average='macro')\n",
        "            \n",
        "            acc_c = accuracy_score(y_test_true.cpu().numpy(), y_pred_c.cpu().numpy())\n",
        "            precision_c, recall_c, fscore_c, _ = score(y_test_true.cpu().numpy(), y_pred_c.cpu().numpy(), average='macro')\n",
        "            \n",
        "            acc_d = accuracy_score(y_test_true.cpu().numpy(), y_pred_d.cpu().numpy())\n",
        "            precision_d, recall_d, fscore_d, _ = score(y_test_true.cpu().numpy(), y_pred_d.cpu().numpy(), average='macro')\n",
        "            \n",
        "            # Average across adversarial sets\n",
        "            acc_res = (acc_a + acc_b + acc_c + acc_d) / 4\n",
        "            precision_res = (precision_a + precision_b + precision_c + precision_d) / 4\n",
        "            recall_res = (recall_a + recall_b + recall_c + recall_d) / 4\n",
        "            fscore_res = (fscore_a + fscore_b + fscore_c + fscore_d) / 4\n",
        "            \n",
        "    # Save model checkpoint\n",
        "    os.makedirs('checkpoints', exist_ok=True)\n",
        "    checkpoint_path = os.path.join('checkpoints', f'{datasetname}_synonym_iter{iter}.m')\n",
        "    torch.save(model.state_dict(), checkpoint_path)\n",
        "    print(f\"üíæ Model saved to: {checkpoint_path}\")\n",
        "\n",
        "    # Print results\n",
        "    print(f\"\\n----------------- End of Iter {iter:03d} -----------------\")\n",
        "    print(\"üéØ Original Test Set:\")\n",
        "    print([f'Global Test Accuracy: {acc:.4f}',\n",
        "           f'Precision: {precision:.4f}',\n",
        "           f'Recall: {recall:.4f}',\n",
        "           f'F1: {fscore:.4f}'])\n",
        "    \n",
        "    print(\"üü¶ Adversarial (A - Objective):\")\n",
        "    print([f'Global Test Accuracy: {acc_a:.4f}',\n",
        "           f'Precision: {precision_a:.4f}',\n",
        "           f'Recall: {recall_a:.4f}',\n",
        "           f'F1: {fscore_a:.4f}'])\n",
        "    \n",
        "    print(\"üü© Adversarial (B - Neutral):\")\n",
        "    print([f'Global Test Accuracy: {acc_b:.4f}',\n",
        "           f'Precision: {precision_b:.4f}',\n",
        "           f'Recall: {recall_b:.4f}',\n",
        "           f'F1: {fscore_b:.4f}'])\n",
        "    \n",
        "    print(\"üü® Adversarial (C - Emotionally Triggering):\")\n",
        "    print([f'Global Test Accuracy: {acc_c:.4f}',\n",
        "           f'Precision: {precision_c:.4f}',\n",
        "           f'Recall: {recall_c:.4f}',\n",
        "           f'F1: {fscore_c:.4f}'])\n",
        "    \n",
        "    print(\"üü• Adversarial (D - Sensational):\")\n",
        "    print([f'Global Test Accuracy: {acc_d:.4f}',\n",
        "           f'Precision: {precision_d:.4f}',\n",
        "           f'Recall: {recall_d:.4f}',\n",
        "           f'F1: {fscore_d:.4f}'])\n",
        "    \n",
        "    print(\"‚ö™ Adversarial (Average across A‚ÄìD):\")\n",
        "    print([f'Global Test Accuracy: {acc_res:.4f}',\n",
        "           f'Precision: {precision_res:.4f}',\n",
        "           f'Recall: {recall_res:.4f}',\n",
        "           f'F1: {fscore_res:.4f}'])\n",
        "\n",
        "    return (\n",
        "        acc, precision, recall, fscore,\n",
        "        acc_res, precision_res, recall_res, fscore_res,\n",
        "        acc_a, precision_a, recall_a, fscore_a,\n",
        "        acc_b, precision_b, recall_b, fscore_b,\n",
        "        acc_c, precision_c, recall_c, fscore_c,\n",
        "        acc_d, precision_d, recall_d, fscore_d\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä 10. Main Training Execution Loop\n",
        "\n",
        "Runs multiple training iterations (with different seeds) to report robust average performance.\n",
        "Logs final metrics to a text file for analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def main():\n",
        "    \"\"\"\n",
        "    Main execution function: runs multiple training iterations and aggregates results.\n",
        "    \"\"\"\n",
        "    datasetname = args.dataset_name\n",
        "    batch_size = args.batch_size\n",
        "    max_len = 512\n",
        "    tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
        "    n_epochs = args.n_epochs\n",
        "    iterations = args.iters\n",
        "\n",
        "    # Store results for all iterations\n",
        "    test_accs = []\n",
        "    prec_all, rec_all, f1_all = [], [], []\n",
        "    test_accs_a, prec_all_a, rec_all_a, f1_all_a = [], [], [], []\n",
        "    test_accs_b, prec_all_b, rec_all_b, f1_all_b = [], [], [], []\n",
        "    test_accs_c, prec_all_c, rec_all_c, f1_all_c = [], [], [], []\n",
        "    test_accs_d, prec_all_d, rec_all_d, f1_all_d = [], [], [], []\n",
        "    test_accs_res = []\n",
        "    prec_all_res, rec_all_res, f1_all_res = [], [], []\n",
        "\n",
        "    print(f\"\\nüöÄ Starting {iterations} training iterations for dataset '{datasetname}'\")\n",
        "\n",
        "    for iter in range(iterations):\n",
        "        set_seed(iter)\n",
        "        metrics = train_model(tokenizer, max_len, n_epochs, batch_size, datasetname, iter)\n",
        "        \n",
        "        (acc, prec, recall, f1,\n",
        "         acc_res, prec_res, recall_res, f1_res,\n",
        "         acc_a, prec_a, recall_a, f1_a,\n",
        "         acc_b, prec_b, recall_b, f1_b,\n",
        "         acc_c, prec_c, recall_c, f1_c,\n",
        "         acc_d, prec_d, recall_d, f1_d) = metrics\n",
        "\n",
        "        # Store metrics\n",
        "        test_accs.append(acc)\n",
        "        prec_all.append(prec)\n",
        "        rec_all.append(recall)\n",
        "        f1_all.append(f1)\n",
        "        \n",
        "        test_accs_res.append(acc_res)\n",
        "        prec_all_res.append(prec_res)\n",
        "        rec_all_res.append(recall_res)\n",
        "        f1_all_res.append(f1_res)\n",
        "        \n",
        "        test_accs_a.append(acc_a)\n",
        "        prec_all_a.append(prec_a)\n",
        "        rec_all_a.append(recall_a)\n",
        "        f1_all_a.append(f1_a)\n",
        "        \n",
        "        test_accs_b.append(acc_b)\n",
        "        prec_all_b.append(prec_b)\n",
        "        rec_all_b.append(recall_b)\n",
        "        f1_all_b.append(f1_b)\n",
        "        \n",
        "        test_accs_c.append(acc_c)\n",
        "        prec_all_c.append(prec_c)\n",
        "        rec_all_c.append(recall_c)\n",
        "        f1_all_c.append(f1_c)\n",
        "        \n",
        "        test_accs_d.append(acc_d)\n",
        "        prec_all_d.append(prec_d)\n",
        "        rec_all_d.append(recall_d)\n",
        "        f1_all_d.append(f1_d)\n",
        "\n",
        "    # Print final aggregated results\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"üìä FINAL RESULTS ACROSS ALL ITERATIONS\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    print(f\"‚úÖ Original Test: Acc={sum(test_accs)/iterations:.4f} | Prec={sum(prec_all)/iterations:.4f} | Rec={sum(rec_all)/iterations:.4f} | F1={sum(f1_all)/iterations:.4f}\")\n",
        "    print(f\"üîµ Adversarial A (Objective): Acc={sum(test_accs_a)/iterations:.4f} | Prec={sum(prec_all_a)/iterations:.4f} | Rec={sum(rec_all_a)/iterations:.4f} | F1={sum(f1_all_a)/iterations:.4f}\")\n",
        "    print(f\"üü¢ Adversarial B (Neutral): Acc={sum(test_accs_b)/iterations:.4f} | Prec={sum(prec_all_b)/iterations:.4f} | Rec={sum(rec_all_b)/iterations:.4f} | F1={sum(f1_all_b)/iterations:.4f}\")\n",
        "    print(f\"üü° Adversarial C (Emotional): Acc={sum(test_accs_c)/iterations:.4f} | Prec={sum(prec_all_c)/iterations:.4f} | Rec={sum(rec_all_c)/iterations:.4f} | F1={sum(f1_all_c)/iterations:.4f}\")\n",
        "    print(f\"üî¥ Adversarial D (Sensational): Acc={sum(test_accs_d)/iterations:.4f} | Prec={sum(prec_all_d)/iterations:.4f} | Rec={sum(rec_all_d)/iterations:.4f} | F1={sum(f1_all_d)/iterations:.4f}\")\n",
        "    print(f\"‚ö™ Adversarial Average: Acc={sum(test_accs_res)/iterations:.4f} | Prec={sum(prec_all_res)/iterations:.4f} | Rec={sum(rec_all_res)/iterations:.4f} | F1={sum(f1_all_res)/iterations:.4f}\")\n",
        "\n",
        "    # Save detailed logs\n",
        "    os.makedirs('logs', exist_ok=True)\n",
        "    log_file = os.path.join('logs', f'log_{datasetname}_{args.model_name}_iter{iterations}.txt')\n",
        "    \n",
        "    with open(log_file, 'a+') as f:\n",
        "        f.write('\\n' + '='*80 + '\\n')\n",
        "        f.write(f\"üìä EXPERIMENT: {datasetname} | Model: {args.model_name} | Iters: {iterations} | Epochs: {n_epochs} | Batch: {batch_size}\\n\")\n",
        "        f.write('='*80 + '\\n\\n')\n",
        "        \n",
        "        f.write('------------- ORIGINAL TEST SET -------------\\n')\n",
        "        f.write(f'All Acc.s: {test_accs}\\n')\n",
        "        f.write(f'All Prec.s: {prec_all}\\n')\n",
        "        f.write(f'All Rec.s: {rec_all}\\n')\n",
        "        f.write(f'All F1.s: {f1_all}\\n')\n",
        "        f.write(f'Average Acc.: {sum(test_accs)/iterations:.4f}\\n')\n",
        "        f.write(f'Average Macro: Prec={sum(prec_all)/iterations:.4f}, Rec={sum(rec_all)/iterations:.4f}, F1={sum(f1_all)/iterations:.4f}\\n\\n')\n",
        "        \n",
        "        f.write('------------- ADVERSARIAL A (OBJECTIVE) -------------\\n')\n",
        "        f.write(f'All Acc.s: {test_accs_a}\\n')\n",
        "        f.write(f'All Prec.s: {prec_all_a}\\n')\n",
        "        f.write(f'All Rec.s: {rec_all_a}\\n')\n",
        "        f.write(f'All F1.s: {f1_all_a}\\n')\n",
        "        f.write(f'Average Acc.: {sum(test_accs_a)/iterations:.4f}\\n')\n",
        "        f.write(f'Average Macro: Prec={sum(prec_all_a)/iterations:.4f}, Rec={sum(rec_all_a)/iterations:.4f}, F1={sum(f1_all_a)/iterations:.4f}\\n\\n')\n",
        "        \n",
        "        f.write('------------- ADVERSARIAL B (NEUTRAL) -------------\\n')\n",
        "        f.write(f'All Acc.s: {test_accs_b}\\n')\n",
        "        f.write(f'All Prec.s: {prec_all_b}\\n')\n",
        "        f.write(f'All Rec.s: {rec_all_b}\\n')\n",
        "        f.write(f'All F1.s: {f1_all_b}\\n')\n",
        "        f.write(f'Average Acc.: {sum(test_accs_b)/iterations:.4f}\\n')\n",
        "        f.write(f'Average Macro: Prec={sum(prec_all_b)/iterations:.4f}, Rec={sum(rec_all_b)/iterations:.4f}, F1={sum(f1_all_b)/iterations:.4f}\\n\\n')\n",
        "        \n",
        "        f.write('------------- ADVERSARIAL C (EMOTIONALLY TRIGGERING) -------------\\n')\n",
        "        f.write(f'All Acc.s: {test_accs_c}\\n')\n",
        "        f.write(f'All Prec.s: {prec_all_c}\\n')\n",
        "        f.write(f'All Rec.s: {rec_all_c}\\n')\n",
        "        f.write(f'All F1.s: {f1_all_c}\\n')\n",
        "        f.write(f'Average Acc.: {sum(test_accs_c)/iterations:.4f}\\n')\n",
        "        f.write(f'Average Macro: Prec={sum(prec_all_c)/iterations:.4f}, Rec={sum(rec_all_c)/iterations:.4f}, F1={sum(f1_all_c)/iterations:.4f}\\n\\n')\n",
        "        \n",
        "        f.write('------------- ADVERSARIAL D (SENSATIONAL) -------------\\n')\n",
        "        f.write(f'All Acc.s: {test_accs_d}\\n')\n",
        "        f.write(f'All Prec.s: {prec_all_d}\\n')\n",
        "        f.write(f'All Rec.s: {rec_all_d}\\n')\n",
        "        f.write(f'All F1.s: {f1_all_d}\\n')\n",
        "        f.write(f'Average Acc.: {sum(test_accs_d)/iterations:.4f}\\n')\n",
        "        f.write(f'Average Macro: Prec={sum(prec_all_d)/iterations:.4f}, Rec={sum(rec_all_d)/iterations:.4f}, F1={sum(f1_all_d)/iterations:.4f}\\n\\n')\n",
        "        \n",
        "        f.write('------------- ADVERSARIAL AVERAGE (A‚ÄìD) -------------\\n')\n",
        "        f.write(f'All Acc.s: {test_accs_res}\\n')\n",
        "        f.write(f'All Prec.s: {prec_all_res}\\n')\n",
        "        f.write(f'All Rec.s: {rec_all_res}\\n')\n",
        "        f.write(f'All F1.s: {f1_all_res}\\n')\n",
        "        f.write(f'Average Acc.: {sum(test_accs_res)/iterations:.4f}\\n')\n",
        "        f.write(f'Average Macro: Prec={sum(prec_all_res)/iterations:.4f}, Rec={sum(rec_all_res)/iterations:.4f}, F1={sum(f1_all_res)/iterations:.4f}\\n')\n",
        "    \n",
        "    print(f\"\\nüìù Log saved to: {log_file}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚ñ∂Ô∏è 11. Execute Training\n",
        "\n",
        "This is the entry point of the script. Runs the main training loop.\n",
        "Only executed if the script is run directly (not imported)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}